<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Lien entre l&#39;apprentissage statistique et le transport optimal | Marco&#39;s website</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Acceuil</a></li>
      
      <li><a href="/about/">À propos</a></li>
      
      <li><a href="/categories/">Catégories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/post/">Blog</a></li>
      
      <li><a href="/index.xml">S&#39;abonner</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Lien entre l&rsquo;apprentissage statistique et le transport optimal</span></h1>
<h2 class="author">Marco</h2>
<h2 class="date">2023/09/17</h2>
</div>

<main>
<p>Dans ce blog, nous illustrons comment le transport optimal entre deux mesures discrètes peut être réduit à un problème d&rsquo;apprentissage statistique qu&rsquo;on peut résoudre à l&rsquo;aide d&rsquo;un algorithme d&rsquo;optimization ici la descente de gradient.</p>
<h2 id="transport-optimal">Transport optimal</h2>
<p>Soit</p>
<ul>
<li><code>$(\Omega_X, F_X, \mu)$</code> et <code>$(\Omega_X, F_Y, \nu)$</code> deux espaces de probabilité</li>
<li><code>$X:\Omega_X \rightarrow \mathbb{R}$</code> et <code>$Y:\Omega_Y \rightarrow \mathbb{R}$</code> deux variables aléatoires</li>
</ul>
<p>La formulation de Kantorovich du problème de transport optimal est :</p>
<p><code>$$ E_1 : \dfrac{\inf}{\gamma \in \Gamma(\mu, \nu)}{ \int_{X \times Y} c_T(x,y)d\gamma(x,y)} $$</code></p>
<h2 id="apprentissage-statistique">Apprentissage statistique</h2>
<p>Soit</p>
<ul>
<li><code>$(x_1, y_1), ..., (x_n, y_n) \sim X \times Y$</code> des échantillons issues de deux variables aléatoires</li>
<li><code>$f:X \rightarrow Y$</code> une fonction qu&rsquo;on aimerait apprendre</li>
</ul>
<p>si <code>$f_h$</code> est un candidat, le coût associé est <code>$c_T(x,y) = d(f_h(x),y)$</code>
et le risque</p>
<p><code>$$ R_T = \int_{X \times Y} c_T(x,y)d\gamma(x,y) : \gamma \in \Gamma(x,y) $$</code></p>
<p>La distribution <code>$\gamma(x,y)$</code> étant inconnue, on se contente donc de minimizer le risque empirique obtenu à l&rsquo;aide de l&rsquo;échantillon. Le risque empirique est donné par</p>
<p><code>$$ R_{emp}^{T} = \frac{1}{N} \sum_{i=1}^{N}c_T(x_i, y_i) $$</code></p>
<h2 id="simulation-numérique">Simulation numérique</h2>
<p>Soient <code>$\mu \sim N(0,1)$</code> et <code>$\nu \sim N(0, \sqrt(2))$</code>.</p>
<p>Nous recherchons un plan de transport <code>$T: X \rightarrow Y$</code> qui mininmize <code>$E_1$</code></p>
<p><img src="images/img1.png" alt=""></p>
<p>Le plan de transport optimal <code>$T$</code> satisfait <code>$T_{\ast}(\mu) = \nu$</code> et est appelé mesure image de <code>$\mu$</code> par <code>$T$</code>.</p>
<p><code>$\mu$</code> et <code>$\nu$</code> étant des mesures discretes sur <code>$\Gamma(\mu, \nu)$</code>, <code>$T$</code> doit vérifier <code>$T(\mu(x)) = \nu(y) \forall x,y \in X \times Y$</code></p>
<p><img src="images/img2.png" alt=""></p>
<p>Choisissons le plan de transport suivant</p>
<p><code>$$T(x) = a * x + b : a,b \in \mathbb{R}$$</code></p>
<p>La fonction de coût est la distance entre les deux mesures soit</p>
<p><code>$$c_T(x, y) = (y_i - T(x_i))^2 = (y_i - a * x_i - b)^2$$</code></p>
<p>Le risque empirique correspondant est</p>
<p><code>$$R_{emp}^{T} = \frac{1}{2N} \sum_{i=1}^{N} (y_i - T(x_i))^2 = \frac{1}{2N} \sum_{i=1}^{N} (y_i - a * x_i - b)^2$$</code></p>
<p>Cette fonction est convexe et différentiable en <code>$a$</code> et <code>$b$</code>, elle admet un minimum qui peut être atteint en suivant la direction du gradient.</p>
<p>Le gradient du risque empirique est</p>
<p><code>$$ \begin{align} \nabla R_{emp}^{T} &amp;= ( \dfrac{\partial R_{emp}^{T}}{\partial a} , \dfrac{\partial R_{emp}^{T}}{\partial b} ) \\ &amp;= ( \frac{2}{N} \sum_{i=1}^{N}x_i(a*x_i + b - y_i), \frac{2}{N} \sum_{i=1}^{N} (a * x_i + b - y_i)) \end{align} $$</code></p>
<p>L&rsquo;algorithme de descende de gradient peut être utilisé pour atteindre ce minimum:</p>
<ol>
<li>Choix de <code>$a_0$</code> et <code>$b_0$</code></li>
<li>Calcul du gradient <code>$\nabla R_{emp}^{T}$</code></li>
<li>Mettre à jour les paramètres
<code>$(a_{k+1} b_{k+1}) = (a_k b_k) - \lambda \nabla R_{emp}^{T}$</code></li>
<li>s&rsquo;arrêter si l&rsquo;algorithme converge</li>
</ol>
<p>On peut vérifier graphiquement le résultat obtenu à l&rsquo;aide de cette méthode:</p>
<p><img src="images/img3.png" alt=""></p>
<p>L&rsquo;image de la mesure <code>$\mu$</code> par <code>$T$</code> s&rsquo;approche de <code>$\nu$</code>.</p>
<p>Un notebook est disponible à l&rsquo;adresse suivant pour réproduire les résultats de cette expérience : <a href="https://github.com/istmarco/optimal-transport/blob/main/learning/Statistical%20learning.ipynb">Statistical learning.ipynb</a>.</p>

</main>

  <footer>
  <script defer src="//yihui.org/js/math-code.js"></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script defer src="//yihui.org/js/center-img.js"></script>

  
  <hr/>
  © <a href="https://istmarco.github.io">Marco</a> 2023 &ndash; 2024 | <a href="https://github.com/istmarco">Github</a>
  
  </footer>
  </body>
</html>

